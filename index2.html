<!DOCTYPE html>
<html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pepedash AI Code Editor</title>
    <style>
        #editor {
            width: 100%;
            height: 610px;
            border: none;
            outline: none;
        }
	
	    #myButton {
			background-color: transparent;
			border: none;
			color: white;
			padding: 10px 20px;
			text-align: center;
			text-decoration: none;
			display: inline-block;
			font-size: 16px;
			margin: 4px 2px;
			cursor: pointer;
		}

        .green-btn {
            background-color: green;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
        }
	     #txtOutput {
            width: 100%;
            height: 610px;
            border: none;
            outline: none;
        }
        #txtMsg {
            width: 100%;
            height: 100px;
            border: none;
            outline: none;
        }

        body {
            background-color: black;
        }
    </style>
    <script>
	function buttonClick(event) {
			if (event.keyCode === 9) {
				document.getElementById("myButton").click();
			}
		}    
	    
        function runCode() {
            var code = document.getElementById("editor").value;
            var output = document.getElementById("output");
            output.innerHTML = code;
        }
        
        window.SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
		var recognition = new window.SpeechRecognition();
		
		recognition.onresult = function(event) {
			var transcript = event.results[0][0].transcript;
			if (transcript.toLowerCase() === "run") {
				document.getElementById("myButton").click();
			}
		}
	    
	    var OPENAI_API_KEY = "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";
var bTextToSpeechSupported = false;
var bSpeechInProgress = false;
var oSpeechRecognizer = null
var oSpeechSynthesisUtterance = null;
var oVoices = null;

function OnLoad() {
    if ("webkitSpeechRecognition" in window) {
    } else {
        //speech to text not supported
        lblSpeak.style.display = "none";
    }

    if ('speechSynthesis' in window) {
        bTextToSpeechSupported = true;

        speechSynthesis.onvoiceschanged = function () {
            oVoices = window.speechSynthesis.getVoices();
            for (var i = 0; i < oVoices.length; i++) {
                selVoices[selVoices.length] = new Option(oVoices[i].name, i);
            }
        };
    }
}

function ChangeLang(o) {
    if (oSpeechRecognizer) {
        oSpeechRecognizer.lang = selLang.value;
        //SpeechToText()
    }
}

function Send() {

    var sQuestion = txtMsg.value;
    if (sQuestion == "") {
        alert("Type in your question!");
        txtMsg.focus();
        return;
    }

    var oHttp = new XMLHttpRequest();
    oHttp.open("POST", "https://api.openai.com/v1/completions");
    oHttp.setRequestHeader("Accept", "application/json");
    oHttp.setRequestHeader("Content-Type", "application/json");
    oHttp.setRequestHeader("Authorization", "Bearer " + OPENAI_API_KEY)

    oHttp.onreadystatechange = function () {
        if (oHttp.readyState === 4) {
            //console.log(oHttp.status);
            var oJson = {}
            if (txtOutput.value != "") txtOutput.value += "\n";

            try {
                oJson = JSON.parse(oHttp.responseText);
            } catch (ex) {
                txtOutput.value += "Error: " + ex.message
            }

            if (oJson.error && oJson.error.message) {
                txtOutput.value += "Error: " + oJson.error.message;
            } else if (oJson.choices && oJson.choices[0].text) {
                var s = oJson.choices[0].text;

                if (selLang.value != "en-US") {
                    var a = s.split("?\n");
                    if (a.length == 2) {
                        s = a[1];
                    }
                }

                if (s == "") s = "No response";
                txtOutput.value += "Chat GPT: " + s;
                TextToSpeech(s);
            }            
        }
    };

    var sModel = selModel.value;// "text-davinci-003";
    var iMaxTokens = 2048;
    var sUserId = "1";
    var dTemperature = 0.5;    

    var data = {
        model: sModel,
        prompt: sQuestion,
        max_tokens: iMaxTokens,
        user: sUserId,
        temperature:  dTemperature,
        frequency_penalty: 0.0, //Number between -2.0 and 2.0  Positive value decrease the model's likelihood to repeat the same line verbatim.
        presence_penalty: 0.0,  //Number between -2.0 and 2.0. Positive values increase the model's likelihood to talk about new topics.
        stop: ["#", ";"] //Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
    }

    oHttp.send(JSON.stringify(data));

    if (txtOutput.value != "") txtOutput.value += "\n";
    txtOutput.value += "Me: " + sQuestion;
    txtMsg.value = "";
}

function TextToSpeech(s) {
    if (bTextToSpeechSupported == false) return;
    if (chkMute.checked) return;

    oSpeechSynthesisUtterance = new SpeechSynthesisUtterance();

    if (oVoices) {
        var sVoice = selVoices.value;
        if (sVoice != "") {
            oSpeechSynthesisUtterance.voice = oVoices[parseInt(sVoice)];
        }        
    }    

    oSpeechSynthesisUtterance.onend = function () {
        //finished talking - can now listen
        if (oSpeechRecognizer && chkSpeak.checked) {
            oSpeechRecognizer.start();
        }
    }

    if (oSpeechRecognizer && chkSpeak.checked) {
        //do not listen to yourself when talking
        oSpeechRecognizer.stop();
    }

    oSpeechSynthesisUtterance.lang = selLang.value;
    oSpeechSynthesisUtterance.text = s;
    //Uncaught (in promise) Error: A listener indicated an asynchronous response by returning true, but the message channel closed 
    window.speechSynthesis.speak(oSpeechSynthesisUtterance);
}

function Mute(b) {
    if (b) {
        selVoices.style.display = "none";
    } else {
        selVoices.style.display = "";
    }
}

function SpeechToText() {

    if (oSpeechRecognizer) {

        if (chkSpeak.checked) {
            oSpeechRecognizer.start();
        } else {
            oSpeechRecognizer.stop();
        }

        return;
    }    

    oSpeechRecognizer = new webkitSpeechRecognition();
    oSpeechRecognizer.continuous = true;
    oSpeechRecognizer.interimResults = true;
    oSpeechRecognizer.lang = selLang.value;
    oSpeechRecognizer.start();

    oSpeechRecognizer.onresult = function (event) {
        var interimTranscripts = "";
        for (var i = event.resultIndex; i < event.results.length; i++) {
            var transcript = event.results[i][0].transcript;

            if (event.results[i].isFinal) {
                txtMsg.value = transcript;
                Send();
            } else {
                transcript.replace("\n", "<br>");
                interimTranscripts += transcript;
            }

            var oDiv = document.getElementById("idText");
            oDiv.innerHTML = '<span style="color: #999;">' + interimTranscripts + '</span>';
        }
    };

    oSpeechRecognizer.onerror = function (event) {

    };
}
	    
    </script>
</head>

<body>

    <div align="center">
        <textarea id="editor" style="background-color: transparent; color: green;">
<!-- Type your Javascript AI code here powered by Pepedash-->
<!-- A lot of AI tool is yet to come, this is just a demo-->
<!-- This is an example code , just edit with your own code and say run or click tab key-->

<html lang="en">
   <head>
      <meta charset="UTF-8" />
      <meta name="viewport"
         content="width=device-width, initial-scale=1.0" />
      <title>Image Classifier using ML5 js</title>
      <script src=
         "https://unpkg.com/ml5@0.4.3/dist/ml5.min.js"></script>
      <script>
         var loadFile = function (event) {
         var image = document.getElementById("image");
         image.src = URL.createObjectURL(event.target.files[0]);
         };
         const classifier = ml5.imageClassifier
         ("MobileNet", modelLoaded);
         
         // When the model is loaded
         function modelLoaded() {
         console.log("Model Loaded!");
         }
            
         function predict() {
         classifier.predict(document.getElementById("image"),
            function (err, results) {
                  alert(results[0].label);
               });
            }
         
      </script>
   </head>
   <body onload="OnLoad()">
      <center>
         <h1 style="color: green;">PEPEDASH AI</h1>
         <b>
         Image Classification using Javascript
         </b>
         </br>
         <img src="" alt="" id="image"
            width="315px" height="200px" />
         </br></br>
         <input type="file" accept="image/*"
            onchange="loadFile(event)"
            name="image" id="file" />
         <button onclick="predict()">Predict</button>
      </center>
   </body>
</html>
    </textarea>
    </div>
    <br>
    <div align="center">
        <button id="myButton" onclick="runCode()"></button>
	    <button id="myButton" onclick="runCode()"></button>
	<script>
		document.addEventListener("keydown", buttonClick);
	</script>
	  <script>
		recognition.start();
	</script>
    </div>
    <div id="output"></div>
    <br>
    <div id="idContainer">
       

       

        <textarea id="txtMsg" style="background-color: transparent; color: green;" placeholder="Ask ChatGPT to code AI models in java" style="color: green;"></textarea>

           <div>
            <button class="green-btn" type="button" onclick="Send()" id="btnSend">Ask</button>
            <label style="color: green;" id="lblSpeak"><input style="color: green;" id="chkSpeak" type="checkbox" onclick="SpeechToText()" />Listen</label>
            <label style="color: green;" id="lblMute"><input style="color: green;" id="chkMute" type="checkbox" onclick="Mute(this.checked)"/>Mute</label>

            <select id="selModel" style="color: green; background-color: black;">
                <option value="text-davinci-003">text-davinci-003</option>
                <option value="text-davinci-002">text-davinci-002</option>
                <option value="code-davinci-002">code-davinci-002</option>
            </select>

            <select id="selLang" onchange="ChangeLang(this)" style="color: green; background-color: black;">
                <option value="en-US">English (United States)</option>
                <option value="fr-FR">French (France)</option>
                <option value="ru-RU">Russian (Russia)</option>
                <option value="pt-BR">Portuguese (Brazil)</option>
                <option value="es-ES">Spanish (Spain)</option>
                <option value="de-DE">German (Germany)</option>
                <option value="it-IT">Italian (Italy)</option>
                <option value="pl-PL">Polish (Poland)</option>
                <option value="nl-NL">Dutch (Netherlands)</option>
            </select>

            <select id="selVoices" style="color: green; background-color: black;"></select>
        </div>
         <textarea id="txtOutput" style="background-color: transparent; color: green;" placeholder="Output" style="color: green;"></textarea>
        <div id="idText"></div>
    </div>

</body>

</html>
